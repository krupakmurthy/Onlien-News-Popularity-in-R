m(list=ls())

#Date: Oct 8th, 2018
#Purpose: Analysis of Online News Popularity

library(randomForest)
  library(caret) # Accuracy
library(ggplot2)
library(ROCR)
library(pROC)
set.seed(85)
summary(dfcla)
library(miscTools)
library(pROC)
library(e1071)
library(C50)
library(rpart)
library(rpart.plot)
library(rattle)
library(RColorBrewer)
library(ggplot2)
library(caTools)
library(rattle)
library(leaps)


data <- read.csv(file="C:/Users/Shivam Pandit/Desktop/Data Science/Project/OnlineNewsPopularity.csv", header=TRUE, sep=",")
dim(data)
names(data)

#summary before cleaning
summary(data)


# Exploratory Data Analysis & Cleaning Data:
# Check for any missing values:
sum(is.na(data))

#Removing outlier
data1=data[!data$n_unique_tokens==701,]
summary(data1)

#Removing non predictive variables 
data2 <- subset( data1, select = -c(url, timedelta, is_weekend ) )

d <- data2

#Combining Plots for EDA
par(mfrow=c(2,3))
for(i in 1:length(data2)){hist(data2[,i],
        xlab=names(data2)[i] , main = paste("[" , i , "]" ,"Histogram of", names(data2)[i])  )}


#check classes of data
sapply(data, class)

names(data2)

#Converting categorical values from numeric to factor - Weekdays
for (i in 30:36){
  data2[,i] <- factor(data2[,i])

}
sapply(data2, class)

#Converting categorical values from numeric to factor - News subjects
for (i in 12:17){
  data2[,i] <- factor(data2[,i])
}

#Checking importance of news subjects(categorical) on shares
for (i in 12:17){
  
  boxplot(log(data2$shares) ~ (data2[,i]), xlab=names(data2)[i] , ylab="shares")
}

#Checking importance of weekdays on news shares
for (i in 30:36){
  
  boxplot(log(data2$shares) ~ (data2[,i]), xlab=names(data2)[i] , ylab="shares")
}
df <- data2

summary(data2)

# generate z-scores using the scale() function
for(i in ncol(df)-1){ 
  df[,i]<-scale(df[,i], center = TRUE, scale = TRUE)
}
df<-data2
summary(df)

#########################################################
# Sampling the dataset into training data and test data:
#############################################################

#Using Linear Model
###################
lm1<-sample(2,nrow(df),replace=TRUE,prob=c(0.7,0.3))

#split test, train set
trainlm<-df[ind==1,]
testlm<-df[ind==2,]


#lm1 <- sample(nrow(df),as.integer(nrow(df)*0.7))
#trainlm = df[lm1,]
#testlm = df[-lm1,]

# Now, we fit a model with all the variables; shares being the dependent variable and all other explanatory variables from the dataset as the predictors.

fit_lm <- lm(shares ~ ., data = trainlm)
plot(fit_lm)

summary(fit_lm)

#taking log of shares


#Using Random Forest
#########################
#rf<-randomForest(shares ~ . ,data=train1.news,ntree = 200, mtry = 6, importance = TRUE, proximity=TRUE)
#summary(rf)
#table(predict(rf), shares)

#predTrain <- predict(rf, TrainSet, type = "class")
# Checking classification accuracy
#table(predTrain, TrainSet$Condition)  

#pred_mlm1 = predict(fit_mlm1, test.news)
#summary(pred_mlm1)

d1 <- df

summary(df$shares)

################################################
#Implementing KNN
#####################################################

# Dataset for classification
dfcla <-df
dfcla$shares <- as.factor(ifelse(dfcla$shares > 1400,1,0))

summary(dfcla$shares)

# Select training data and prediction data
ind<-sample(2,nrow(dfcla),replace=TRUE,prob=c(0.7,0.3))

#split test, train set
trainData<-dfcla[ind==1,]
testData<-dfcla[ind==2,]


dfcla.knn <- knn3(shares ~.,dfcla[ind==1,])
dfcla.knn.pred <- predict( dfcla.knn,dfcla[ind==2,],type="class")
dfcla.knn.prob <- predict( dfcla.knn,dfcla[ind==2,],type="prob")

# Confusion matrix
confusionMatrix(dfcla.knn.pred, dfcla[ind==2,]$shares)


dfcla.cart<-rpart(shares ~.,dfcla[ind==1,],method='class')

# Standardize data
# generate z-scores using the scale() function
for(i in ncol(dfcla)-1){ 
  dfcla[,i]<-scale(dfcla[,i], center = TRUE, scale = TRUE)

# ROC Curve
par(mfrow=c(1,1))
color.knn<-'#ef696a'
dfcla.knn.roc <- roc(dfcla[ind==2,]$shares,dfcla.knn.prob[,2])
plot(dfcla.knn.roc, print.auc=TRUE, auc.polygon=TRUE, grid=c(0.1, 0.2),
     grid.col=c("green", "red"), max.auc.polygon=TRUE,
     auc.polygon.col=color.knn, print.thres=TRUE)
}

##########################################################
###       Implementing CART
#########################################################
# Define articles with shares larger than 1400 (median) as popular article
# Dataset for classification
newscart <- dfcla
summary(newscart$shares)


# Split Data
# Train - 80%; Test - 20%
#set random situation
set.seed(100)
# Select traning data and prediction data
splitdata <- sample.split(newscart,SplitRatio = 0.65)
train_data <- subset(newscart, splitdata == TRUE)
test_data <- subset(newscart, splitdata == FALSE)
print(train_data)


# Classification and Regression Trees
news.cart<-rpart(shares ~.,newscart[ind==1,],method='class')
print(news.cart)

# Plot tree
fancyRpartPlot(news.cart)

#predict
newscla.cart.pred<-predict( news.cart,newscart[ind==2,] ,type="class")
newscla.cart.prob<-predict( news.cart,newscart[ind==2,] ,type="prob")

# Confusion matrix
confusionMatrix(newscla.cart.pred, newscart[ind==2,]$shares)
